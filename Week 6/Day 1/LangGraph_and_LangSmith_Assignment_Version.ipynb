{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1: LangSmith Evaluator:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "  - Part 2:\n",
        "    3. Adding Helpfulness Check and \"Loop\" Limits\n",
        "    4. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "3b97db0d-d119-4b43-b964-47291c7dba1c"
      },
      "outputs": [],
      "source": [
        "! pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "761167a9-b570-421b-eb9c-8be3dc813f47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b0237b19-ada7-4836-edc2-228e694a5ecb"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE3 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/fab950acfbf5fea46c9313dca34ee2ae01f1728b/libs/langgraph/langgraph/prebuilt/tool_executor.py#L50) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "In Langraph, the model's decision on which tool to use typically involves a process called \"tool selection\" or \"action selection.\" This process is usually handled by an agent, which is a higher-level abstraction that combines a language model with a decision-making framework. Here's how it generally works:\n",
        "\n",
        "Prompt Engineering:\n",
        "The system prompt given to the model includes descriptions of available tools and their functions. This informs the model about what tools are at its disposal.\n",
        "Task Analysis:\n",
        "The model analyzes the user's query or the current state of the task to determine what kind of action or information is needed.\n",
        "Tool Matching:\n",
        "Based on its understanding of the task and the available tools, the model tries to match the current need with the most appropriate tool.\n",
        "Decision Making:\n",
        "The model generates a response that includes its reasoning about which tool to use and why. \n",
        "Execution:\n",
        "The system then executes the chosen tool with the provided input.\n",
        "Observation:\n",
        "The result of the tool's execution is fed back to the model as an observation.\n",
        "Iteration:\n",
        "The model then decides if the task is complete or if it needs to use another tool based on the new information.\n",
        "\n",
        "In LangGraph specifically, this process can be implemented using a state machine where each state represents a different stage of the decision-making process. The transitions between states are determined by the model's outputs and the results of tool executions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(workflow.compile().get_graph().draw_mermaid_png()))#state_update_bot\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)\n",
        "#------------------------------------\n",
        "\n",
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(workflow.get_graph().draw_mermaid_png()))#state_update_bot\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(workflow.compile().get_graph().draw_mermaid_png()))#state_update_bot\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: On my vscode, until the time I had not my edges added, it was not displayed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
          ]
        }
      ],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")\n",
        "# from \"action\" node back to \"agent\" node that makes decion either continue or finishes the cycle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Image(workflow.compile().get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle? No. we have the potential situation of never exiting the loop.\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?  We can add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop. len(state[\"messages\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "90f7d3dc-0fe2-4d1f-8221-9b3bcffc982e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Agent Response: RAG, or Retrieval-Augmented Generation, is a method used in large language models to enhance their ability to generate responses. It combines the benefits of retriever models, which are good at using a large corpus of documents to find relevant information, and seq2seq models, which are good at generating fluent text.\n",
            "\n",
            "In the context of large language models, RAG allows the model to use a large corpus of documents to generate responses. It does this by first retrieving relevant documents from the corpus using a retriever model, and then using a seq2seq model to generate a response based on the retrieved documents.\n",
            "\n",
            "RAG was introduced by Facebook AI in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" which was published in October 2020. The model demonstrated state-of-the-art performance on several knowledge-intensive tasks, showing the effectiveness of combining retrieval and generation in large language models.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "6ae3afec-7da7-4c5d-a3ca-2ed75735c029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Let's jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights Large Language Models (LLMs) are currently a hot topic in the field of machine learning. Imagine you're an ML Engineer and your company has access to GPUs and open-source LLMs like LLAMA/Falcon ... Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Unlock the power of QLoRA with our definitive guide! Learn how to fine-tune the Falcon-7b model using PEFT for optimal AI performance. Step into the future of machine learning today. Large Language Models (LLMs) are currently a hot topic in the field of machine learning. Imagine you're an ML Engineer and your company has access to GPUs and open-source LLMs like LLAMA/Falcon. ... LoRA, Machine Learning, QLoRA, transformers. Categories: data-science, machine-learning. Updated: July 26, 2023. Share on Twitter Facebook ...\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2024-05-27\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technologies derived from the perspective of unified information: (1)\n",
            "statistics-based Information Calibration Quantization allows the quantized\n",
            "parameters of LLM to retain original information accurately; (2)\n",
            "finetuning-based Information Elastic Connection makes LoRA utilizes elastic\n",
            "representation transformation with diverse information. Comprehensive\n",
            "experiments show that IR-QLoRA can significantly improve accuracy across LLaMA\n",
            "and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\n",
            "improvement on MMLU compared with the state-of-the-art methods. The significant\n",
            "performance gain requires only a tiny 0.31% additional time consumption,\n",
            "revealing the satisfactory efficiency of our IR-QLoRA. We highlight that\n",
            "IR-QLoRA enjoys excellent versatility, compatible with various frameworks\n",
            "(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\n",
            "The code is available at https://github.com/htqin/ir-qlora.\n",
            "\n",
            "Published: 2023-12-31\n",
            "Title: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\n",
            "Authors: Dipankar Sarkar\n",
            "Summary: This paper aims to introduce and analyze the Viz system in a comprehensive\n",
            "way, a novel system architecture that integrates Quantized Low-Rank Adapters\n",
            "(QLoRA) to fine-tune large language models (LLM) within a l\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Episode 82 of the Stanford MLSys Seminar Series!Democratizing Foundation Models via k-bit QuantizationSpeaker: Tim DettmersAbstract:Foundation models are eff... Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\n",
            "\n",
            "Agent Response: QLoRA stands for \"Quantized Low-Rank Adapters\" and is an efficient finetuning approach that reduces memory usage while preserving full 16-bit finetuning task performance for large language models (LLMs). QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The best model family resulting from QLoRA, named Guanaco, outperforms all previous openly released models on the Vicuna benchmark.\n",
            "\n",
            "The technical paper \"QLoRA: Efficient Finetuning of Quantized LLMs\" was authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Tim Dettmers is a researcher who focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. He works on developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\n",
            "\n",
            "If you would like more information or details on the QLoRA paper or Tim Dettmers, please let me know!\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "IT BREAKS THE COMPLEX QUERY, RETRIEVS RELAVANT INFORMATION USING THE APPROPRIATE TOOLS. THEN, GEBERATE RESPONSE BY HAVING THE KNOWLEDGE GATHERRED.\n",
        "\n",
        "For the Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
        "\n",
        "1. What is QLoRA in Machine Learning? -> Tool Call - Name: duckduckgo_search + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
        "2. Are their any technical papers that could help me understand? -> Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA in Machine Learning\"}:\n",
        "THREE research papers retrieved from arxiv.\n",
        "3. Once you have that information, can you look up the bio of the first author on the QLoRA paper? -> Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
        "\n",
        "4. Agent Response: QLoRA stands for \"Quantized Low-Rank Adapters\" and is an efficient finetuning approach that reduces memory usage while preserving full 16-bit finetuning task performance for large language models (LLMs). QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The best model family resulting from QLoRA, named Guanaco, outperforms all previous openly released models on the Vicuna benchmark.\n",
        "\n",
        "The technical paper \"QLoRA: Efficient Finetuning of Quantized LLMs\" was authored by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Tim Dettmers is a researcher who focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. He works on developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning.\n",
        "\n",
        "If you would like more information or details on the QLoRA paper or Tim Dettmers, please let me know!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "d2b3be9d-1dbb-4109-83e4-673c9b73483e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG, or retrieval-augmented generation, combines retrieval and generation models to enhance text quality and relevance. It is a technique that enhances generative AI models with facts from external sources. RAG works by grounding the model on external sources of knowledge to improve the quality of generated responses. It is used in various NLP tasks to provide more accurate and relevant information.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is the key innovation in the PEFT (Parameter-Efficient Fine-Tuning) approach?\",\n",
        "    \"How does the Mixtral 8x7B model differ from traditional large language models?\",\n",
        "    \"What is the main advantage of using Retrieval-Augmented Generation in language models?\",\n",
        "    \"What is the primary purpose of the RLHF (Reinforcement Learning from Human Feedback) technique?\",\n",
        "    \"How does the GPT (Generative Pre-trained Transformer) architecture revolutionize natural language processing?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\": [\"adapter layers\", \"efficiency\"]},\n",
        "    {\"must_mention\": [\"mixture of experts\", \"sparse activation\"]},\n",
        "    {\"must_mention\": [\"up-to-date information\", \"reduced hallucinations\"]},\n",
        "    {\"must_mention\": [\"align model\", \"human preferences\"]},\n",
        "    {\"must_mention\": [\"self-attention mechanism\", \"transfer learning\"]}\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the Advanced developments and techniques in artificial intelligence, Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions? It sounds like it focuses on key words rather than more semantic approach.\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or : Yes! it is problematic.\n",
        "\n",
        "1. Limitations of keyword-based matching: (\n",
        "Lacks understanding of context and nuance\n",
        "May miss semantically correct answers that use different terminology\n",
        "Can lead to false positives if keywords appear without proper context)\n",
        "\n",
        "\n",
        "2. Oversimplification of complex topics: (\n",
        "Reducing answers to a few keywords doesn't capture the depth of understanding required\n",
        "Fails to account for the explanatory nature of many questions in this field)\n",
        "\n",
        "\n",
        "3. Potential for gaming the system: (\n",
        "Encourages memorization of keywords rather than true comprehension\n",
        "May lead to shallow learning focused on buzzwords)\n",
        "\n",
        "\n",
        "4. Lack of flexibility: (\n",
        "Doesn't allow for alternative explanations or approaches\n",
        "May not recognize valid answers that use synonyms or related concepts\n",
        ")\n",
        "\n",
        "5. Misalignment with the nature of the field: (\n",
        "Advanced NLP and ML often require nuanced understanding that goes beyond simple keyword matching\n",
        "The field evolves rapidly, and fixed keywords may quickly become outdated)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "A more robust approach would involve:\n",
        "\n",
        "1. Semantic understanding of questions and answers\n",
        "2. Consideration of context and relationships between concepts\n",
        "3. Allowance for multiple valid ways of expressing the same idea\n",
        "4. Evaluation of the overall coherence and accuracy of the answer, not just presence of keywords\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\", #helpfulness\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "157f1019-4ee4-4994-fe80-7afc2e5fdfb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 2b6478e2' at:\n",
            "https://smith.langchain.com/o/011bb40b-887f-5e52-ae2b-cac14745aa21/datasets/558de6f2-5e92-4afe-87e5-eb5070e57d80/compare?selectedSessions=68735500-6101-4666-8ad7-8880e73a3573\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - a775505b at:\n",
            "https://smith.langchain.com/o/011bb40b-887f-5e52-ae2b-cac14745aa21/datasets/558de6f2-5e92-4afe-87e5-eb5070e57d80\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Evaluation with the <class 'langchain.evaluation.criteria.eval_chain.CriteriaEvalChain'> requires a language model to function. Failed to create the default 'gpt-4' model. Please manually provide an evaluation LLM or check your openai credentials.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langchain/evaluation/loading.py:148\u001b[0m, in \u001b[0;36mload_evaluator\u001b[0;34m(evaluator, llm, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import langchain_openai or fallback onto \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_community. Please install langchain_openai \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecify a language model explicitly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m             )\n\u001b[0;32m--> 148\u001b[0m     llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Parameters {'seed'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter. (type=value_error)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# with 5 created questions\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# convert_inputs | app | parse_output\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRAG Pipeline - Evaluation - \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muuid4\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mversion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langsmith/client.py:4548\u001b[0m, in \u001b[0;36mClient.run_on_dataset\u001b[0;34m(self, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, dataset_version, verbose, input_mapper, revision_id, **kwargs)\u001b[0m\n\u001b[1;32m   4543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   4544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   4545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe client.run_on_dataset function requires the langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage to run.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstall with pip install langchain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4547\u001b[0m     )\n\u001b[0;32m-> 4548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_on_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcurrency_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4557\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4560\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4561\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:1374\u001b[0m, in \u001b[0;36mrun_on_dataset\u001b[0;34m(client, dataset_name, llm_or_chain_factory, evaluation, dataset_version, concurrency_level, project_name, project_metadata, verbose, revision_id, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     warn_deprecated(\n\u001b[1;32m   1367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1368\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following arguments are deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0.305\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1372\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m client \u001b[38;5;241m=\u001b[39m client \u001b[38;5;129;01mor\u001b[39;00m Client()\n\u001b[0;32m-> 1374\u001b[0m container \u001b[38;5;241m=\u001b[39m \u001b[43m_DatasetRunContainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_or_chain_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concurrency_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1388\u001b[0m     batch_results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1389\u001b[0m         _run_llm_or_chain(\n\u001b[1;32m   1390\u001b[0m             example,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(container\u001b[38;5;241m.\u001b[39mexamples, container\u001b[38;5;241m.\u001b[39mconfigs)\n\u001b[1;32m   1396\u001b[0m     ]\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:1194\u001b[0m, in \u001b[0;36m_DatasetRunContainer.prepare\u001b[0;34m(cls, client, dataset_name, llm_or_chain_factory, project_name, evaluation, tags, input_mapper, concurrency_level, project_metadata, revision_id, dataset_version)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     run_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision_id\n\u001b[1;32m   1193\u001b[0m wrapped_model \u001b[38;5;241m=\u001b[39m _wrap_in_chain_factory(llm_or_chain_factory)\n\u001b[0;32m-> 1194\u001b[0m run_evaluators \u001b[38;5;241m=\u001b[39m \u001b[43m_setup_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1197\u001b[0m _validate_example_inputs(examples[\u001b[38;5;241m0\u001b[39m], wrapped_model, input_mapper)\n\u001b[1;32m   1198\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m progress\u001b[38;5;241m.\u001b[39mProgressBarCallback(\u001b[38;5;28mlen\u001b[39m(examples))\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:439\u001b[0m, in \u001b[0;36m_setup_evaluation\u001b[0;34m(llm_or_chain_factory, examples, evaluation, data_type)\u001b[0m\n\u001b[1;32m    437\u001b[0m         run_inputs \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minput_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chain, Chain) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m         run_outputs \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39moutput_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chain, Chain) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     run_evaluators \u001b[38;5;241m=\u001b[39m \u001b[43m_load_run_evaluators\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# TODO: Create a default helpfulness evaluator\u001b[39;00m\n\u001b[1;32m    449\u001b[0m     run_evaluators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:624\u001b[0m, in \u001b[0;36m_load_run_evaluators\u001b[0;34m(config, run_type, data_type, example_outputs, run_inputs, run_outputs)\u001b[0m\n\u001b[1;32m    620\u001b[0m     input_key, prediction_key, reference_key \u001b[38;5;241m=\u001b[39m _get_keys(\n\u001b[1;32m    621\u001b[0m         config, run_inputs, run_outputs, example_outputs\n\u001b[1;32m    622\u001b[0m     )\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eval_config \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mevaluators:\n\u001b[0;32m--> 624\u001b[0m     run_evaluator \u001b[38;5;241m=\u001b[39m \u001b[43m_construct_run_evaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprediction_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m     run_evaluators\u001b[38;5;241m.\u001b[39mappend(run_evaluator)\n\u001b[1;32m    635\u001b[0m custom_evaluators \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mcustom_evaluators \u001b[38;5;129;01mor\u001b[39;00m []\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:536\u001b[0m, in \u001b[0;36m_construct_run_evaluator\u001b[0;34m(eval_config, eval_llm, run_type, data_type, example_outputs, reference_key, input_key, prediction_key)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_config, EvaluatorType):\n\u001b[1;32m    535\u001b[0m         eval_config \u001b[38;5;241m=\u001b[39m EvaluatorType(eval_config)\n\u001b[0;32m--> 536\u001b[0m     evaluator_ \u001b[38;5;241m=\u001b[39m \u001b[43mload_evaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     eval_type_tag \u001b[38;5;241m=\u001b[39m eval_config\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_config, smith_eval_config\u001b[38;5;241m.\u001b[39mEvalConfig):\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langchain/evaluation/loading.py:152\u001b[0m, in \u001b[0;36mload_evaluator\u001b[0;34m(evaluator, llm, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m ChatOpenAI(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    149\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m}, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    150\u001b[0m         )\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation with the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator_cls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage model to function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Failed to create the default \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please manually provide an evaluation LLM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or check your openai credentials.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluator_cls\u001b[38;5;241m.\u001b[39mfrom_llm(llm\u001b[38;5;241m=\u001b[39mllm, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mValueError\u001b[0m: Evaluation with the <class 'langchain.evaluation.criteria.eval_chain.CriteriaEvalChain'> requires a language model to function. Failed to create the default 'gpt-4' model. Please manually provide an evaluation LLM or check your openai credentials."
          ]
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name, # with 5 created questions\n",
        "    llm_or_chain_factory=agent_chain, # convert_inputs | app | parse_output\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "we used the `built-in-evaluator` helfulness before. Now, we define function:\n",
        "1. first message in state is the input query by user, last massage in state is the answer generate by agent.\n",
        "2. The helfulness check model (gpt4) is in a role to examine and decides if the generated reponse is extremely helpful or not.\n",
        "3. If `Yes`,it returns end and later agent ends, otherwise, it returns `continues`, and later agent takes action to call tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "# node agent has access to call model.\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "# node action has access to call tools at its disposal.\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)\n",
        "# node passthrough "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        " SETTING THE GRAPH WORKFLOW: \n",
        "1. AN INSTANCE OF THE STATEGRAPH CREATED OF THE TYPE AgentState. THREE NODES ADDED TO THE GRAPH USING add_node() method:\n",
        "2. THE \"agent\" NODE IS ASSOCIATED WITH THE call_model FUNCTION.\n",
        "3. THE \"action\" NODE IS ASSOCIATED WITH THE call_tool FUNCTION.\n",
        "4. THE \"passthrough\" NODE IS A CUSTOM NODE THAT IS ASSOCIATED WITH CHECKING HELPFULNESS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. \"agent\" NODE IS SET AS THE ENTRY POINT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    check_helpfulness,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Image(graph_with_helpfulness_check.compile().get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "6. FROM \"agent\" NODE, CONDITIONAL EDGE TO EITHER \"action\" NODE (IF \"function_call\" IN LAST MESSAGE IN STATE) OR \"passthrough\" NODE (IF END IS CALLED).\n",
        "7. FROM \"passthrough\" NODE, THE CRITERIA IS CHECKED. IF MET A CONDITIONAL EDGE TO \"end\" NODE, OTHERWISE, CONTINUE BACK TO \"agent\" NODE FOR FURTHER TAKIND ACTION."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8. FROM \"action\" NODE BACK TO \"agent\" NODE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Image(graph_with_helpfulness_check.compile().get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### YOUR MARKDOWN HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "72096ca6-e78e-475a-dde5-fe076c5b776d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not helpful!\n",
            "Not helpful!\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'END'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[81], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m : [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelated to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]}\n\u001b[0;32m----> 5\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[43magent_with_helpfulness_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m print_messages(messages)\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1691\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1690\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1693\u001b[0m     config,\n\u001b[1;32m   1694\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1695\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1696\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m   1697\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1698\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1699\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1701\u001b[0m ):\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1703\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1110\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1110\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1780\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1778\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1779\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1783\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langgraph/pregel/retry.py:72\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     70\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langchain_core/runnables/base.py:2824\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2822\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2823\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2824\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2826\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langgraph/graph/graph.py:82\u001b[0m, in \u001b[0;36mBranch._route\u001b[0;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[1;32m     80\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m     81\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minvoke(value, config)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langgraph/graph/graph.py:109\u001b[0m, in \u001b[0;36mBranch._finish\u001b[0;34m(self, writer, input, result)\u001b[0m\n\u001b[1;32m    107\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[0;32m--> 109\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends[r] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m result\n",
            "File \u001b[0;32m~/anaconda3/envs/ai3/lib/python3.10/site-packages/langgraph/graph/graph.py:109\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[0;32m--> 109\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m result\n",
            "\u001b[0;31mKeyError\u001b[0m: 'END'"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "1c29765b-42fc-449f-dce4-62dcb747a03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: What is prompt engineering and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\n",
            "  \"query\": \"Prompt engineering history\"\n",
            "}\n",
            "Tool Response: This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding ... Prompt Engineering didn't emerge overnight; its roots can be traced back to the early days of natural language processing and machine learning. Prompt engineering has moved far beyond simple instructions. Practitioners are meticulously crafting prompts to elicit specific responses and ensure that LLMs align with human goals and values. Looking ahead, it's clear we need to think critically about aligning models not just with the 'average' user's preferences, but with a wider range of social and technical perspectives. This article provides an overview of prompt engineering, from its inception to current status. Abstract Prompt design and engineering has rapidly become essential for maximizing the potential of large language models. In this paper, we introduce core concepts, advanced techniques like Chain-of-Thought and Reflection, and the principles behind building LLM-based agents. Finally, we provide a survey of tools for prompt engineers.\n",
            "\n",
            "Agent Response: Prompt engineering is a technique used in the field of natural language processing (NLP) to maximize the potential of large language models (LLMs). It involves meticulously crafting prompts to elicit specific responses and ensure that LLMs align with human goals and values. The roots of prompt engineering can be traced back to the early days of NLP and machine learning, but it has evolved significantly over the years.\n",
            "\n",
            "The introduction of attention mechanisms in 2015 was a key development that revolutionized language understanding and contributed to the evolution of prompt engineering. Advanced techniques like Chain-of-Thought and Reflection have been introduced, and there are now various tools available for prompt engineers.\n",
            "\n",
            "While the exact date when prompt engineering \"broke onto the scene\" is not specified, it seems to have gained prominence with the advent and evolution of more sophisticated language models in recent years.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is RAG and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\n",
            "  \"query\": \"RAG definition and history\"\n",
            "}\n",
            "Tool Response: Retrieval-augmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources. To understand the latest advance in generative AI, imagine a courtroom. Judges hear and decide cases based on their general understanding of the law. RAG is a framework that combines large language models like GPT-3 with external data sources. It works by first retrieving relevant information from databases or documents, and then using that information to generate better responses. For example, imagine you asked a RAG-powered chatbot \"What's the capital of France?\". If you're looking for a non-technical introduction to RAG, including answers to various getting-started questions and a discussion of relevant use-cases, check out our breakdown of RAG here.. In ... Simply put, RAG is a prompt engineering technique used to enhance the output of a Large Language Model (LLM). This is achieved by retrieving additional information from a knowledge base external to the LLM for the purpose of augmenting the prompt input provided to the LLM.Ultimately, these two steps lead to a generation of output far superior than if the LLM had acted alone. Introduction. One of the most popular themes currently around Large Language Models is the idea of Retrieval Augmented Generation (RAG). RAG is important for a number of reasons, in this article I first discuss what caused the rise in popularity of RAG. Together with a basic illustration of RAG in its most simplest form.\n",
            "\n",
            "Agent Response: Retrieval-augmented generation (RAG) is a technique used to enhance the output of large language models (LLMs). It works by retrieving additional information from an external knowledge base to augment the input provided to the LLM, leading to a generation of output that is superior to what the LLM could produce alone.\n",
            "\n",
            "RAG combines large language models like GPT-3 with external data sources. It first retrieves relevant information from databases or documents, and then uses that information to generate better responses. For example, if you asked a RAG-powered chatbot \"What's the capital of France?\", it would use its external knowledge base to ensure the accuracy of its response.\n",
            "\n",
            "The exact time when RAG broke onto the scene is not specified in the search results. However, it is a relatively recent development in the field of AI and has gained popularity due to its ability to enhance the performance of generative models.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is fine-tuning and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Fine-tuning in the context of machine learning refers to a process where a pre-trained model, such as a neural network model that has been trained on a large-scale dataset, is adapted for a specific task. This is done by continuing the training of the model on a smaller, task-specific dataset. The idea is that the model has already learned general features from the large dataset, and can refine this knowledge for the specific task.\n",
            "\n",
            "Fine-tuning is particularly useful in scenarios where the task-specific dataset is small, and training a model from scratch on this dataset might lead to overfitting.\n",
            "\n",
            "As for when fine-tuning broke onto the scene, it's hard to pinpoint an exact date as the concept has evolved over time. However, the technique became more prominent with the advent of deep learning and large-scale datasets like ImageNet around 2012. The success of transfer learning models like BERT in NLP, which heavily rely on fine-tuning, further popularized the technique in recent years.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is LLM-based agents and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\n",
            "  \"query\": \"LLM-based agents\"\n",
            "}\n",
            "Tool Response: Fig. 2: An LLM-based agent interacts with its environment through perception, sensing environmental data, and takes action based on the information, which may involve tools. In unimodal mode, the ... In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain ... Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in ... There are two types of memory modules: Short-term memory: A ledger of actions and thoughts that an agent goes through to attempt to answer a single question from a user: the agent's \"train of thought.\". Long-term memory: A ledger of actions and thoughts about events that happen between the user and agent. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering.\n",
            "\n",
            "Agent Response: LLM-based agents refer to Large Language Models (LLMs) that have been used as autonomous agents to perform a variety of tasks automatically. These agents interact with their environment through perception, sensing environmental data, and take action based on the information, which may involve tools. They have been used in a wide array of tasks due to their impressive planning and reasoning abilities. \n",
            "\n",
            "Recently, LLM-based multi-agent systems have achieved considerable progress. These agents have two types of memory modules: short-term memory, which is a ledger of actions and thoughts that an agent goes through to attempt to answer a single question from a user, and long-term memory, which is a ledger of actions and thoughts about events that happen between the user and agent.\n",
            "\n",
            "LLM-based agents have diverse applications in the fields of social science, natural science, and engineering. The exact time when LLM-based agents broke onto the scene is not specified in the search results. For that, I would need to perform another search. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print_messages(messages)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
